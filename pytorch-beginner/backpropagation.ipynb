{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that backpropagation is not a PyTorch-specific concept. It is a general concept that applies to any deep learning framework. We will use PyTorch to implement backpropagation.\n",
    "\n",
    "Backpropagation is a method to calculate the gradient of the loss function with respect to the weights of the neural network. The gradient is used to update the weights of the neural network using gradient descent. \n",
    "\n",
    "Backpropagation is the heart of deep learning. It is the method by which the neural network learns. It works by using the chain rule of calculus to calculate the gradient of the loss function with respect to the weights of the neural network. The gradient is then used to update the weights of the neural network using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses automatic differentiation to calculate the gradient of the loss function with respect to the weights of the neural network. Automatic differentiation is a method to calculate the derivative of a function. It is a general concept that applies to any deep learning framework. We will use PyTorch to implement automatic differentiation.\n",
    "\n",
    "PyTorch uses a technique called dynamic computational graphs to calculate the gradient of the loss function with respect to the weights of the neural network. A dynamic computational graph is a graph that is created at runtime. The graph is created by tracing the code that calculates the output of the neural network. The graph is then used to calculate the gradient of the loss function with respect to the weights of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass and compute the loss\n",
    "y_hat = w * x                       # Function: y_hat = w * x\n",
    "loss = (y_hat - y)**2               # Loss: (y_hat - y)**2\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss.backward()                     # Compute gradients\n",
    "\n",
    "# Print out the gradients\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "# w.data = w.data - 0.01 * w.grad.data\n",
    "# Next forward & Backward Pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "715289c5c08396236a33167d9acf491181f40a998edd45e8743b3070e480bded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
